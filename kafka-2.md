**kafka 的zookeeper迁移**
需要修改zookeeper地址的地方：
schmea-registry, brokers, consumers
原zk节点:
```
server.4=10.132.165.11:2888:3888

server.2=10.153.201.220:2888:3888

server.1=10.153.203.22:2888:3888

server.3=10.132.164.136:2888:3888

server.5=10.30.197.149:2888:3888
```
迁移步骤：

      1. 使用命令echo srvr |nc 127.0.0.1 2181查看5节点集群状态，以及各个节点的角色
      2. 备份zk中的数据（虽然原则上为不可回滚操作，安全起见还是备份一下数据）
      扩容： 新zk节点为扩容6，7，8
      3. 先修改6，7节点的zoo.cfg配置
         server.1=10.10.0.110:2888:3888
         server.2=10.10.0.111:2888:3888
         server.3=10.10.0.112:2888:3888
         server.4=10.10.0.113:2888:3888
         server.5=10.10.0.115:2888:3888
         server.6=...
         server.7=...
      4.依次重启6，7节点,并在leader节点查看数据是否同步(echo mntr|nc 10.30.197.149 2181)，此时确定可以看到6，7节点已经从leader同步到数据，且为正常读写节点
      5. 1，2，3，4，5节点不做任何变更
      6.1 此时需要所有kafka broker节点新的zk节点6，7，并依次重启节点，间隔5分钟
      6.2  schema-registry,consumers用到kafka的zk也需要更改到的新的zk地址 

      7. 此时8节点暂时不起动，kafka所有节点指向新的zk节点6，7
      缩容:  将6，7，8节点变为新的集群，给kafka独立使用
      8. 修改6，7，8节点的zoo.cfg配置为：
         server.6=10.10.0.115:2888:3888
         server.7=...
         server.8=...
      9.  此时启动节点8，但是8为looking状态，并不会加入集群，且数据为空
      10. 重启节点6，7节点这个时候，6，7的zxid高于8，进行选举，6，7其中一个为leader，6，7，8为新集群 （注意： 测试过程此时如果只重启6，7其中一个节点，会导致老集群中/kafka/brokers/ids列表为空，同步到新集群，导致kafka需要再依次重启，所以6，7应该同时重启zk，会有重启时间段zk不可用，但kafka生产消费不依赖zk的正常）
      11.至此，kafka已经将zk迁移到新的zk集群，并不是完全平滑，中间有短时间zk不可用



总结：
       在不中断业务的情况下，kafka的zk迁移到新zk集群，保证老zk集群不能下架，还是很棘手的操作。为了降低中断时间最少，此方案测试过程中是zk短时间不可用，但kafka集群一直处于正常工作状态。网上有类似的案例： [不停止kafka，迁移zk](https://engineeringblog.yelp.com/2019/01/migrating-kafkas-zookeeper-with-no-downtime.html)，仔细阅读此案例，依然会有短暂的新zk集群不可用的状态
